## Whole Pipeline

This proposal pipeline follow [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3).
Based on [the original perception architecture figure](https://github.com/scepter914/autoware-radar-architecture-proposal/blob/main/perception/figure/perception_pipeline_original.drawio.svg) drawn by draw.io, I expand sensor fusion architecture to it for radars.

![The pipeline figure for design document](https://raw.githubusercontent.com/scepter914/autoware-radar-architecture-proposal/main/perception/figure/perception_pipeline.drawio.svg)

This figure put on <https://github.com/scepter914/autoware-radar-architecture-proposal/blob/main/perception/figure/perception_pipeline.drawio.svg>

According to radar output types, I suggest two sensor fusion types in perception module.

- Radar fusion in object layer in tracking module
- Radar fusion in `RadarScan` layer in detection module

## Radar fusion in object layer in tracking module
### Fusion tracking

In fusion tracking, radar tracked objects are fused for other tracked objects in tracking module.
As discussed in [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3), fusion tracking module handles tracked objects as below.

- Detected 3d objects from Camera-LiDAR-Radar fusion
- LiDAR tracked 3d objects
- Camera tracked 3d objects
- Radar tracked 3d objects

### Radar tracked objects

Radar tracked objects are generated by two types.

- Radar driver

If you have radar driver which can publish tracked objects calculated inside the radar devices, then you could enter radar outputs directly to fusion tracking.
Tracked object output can make sensor fusion pipeline simpler.

- Radar 3d detection and tracking

If you have radar driver which publish [`RadarScan`](https://github.com/ros-perception/radar_msgs/blob/ros2/msg/RadarScan.msg), then you could use  radar tracking module, which calculates tracked objects from `RadarScan`.
The tracked objects generated by radar tracking module can use for fusion tracking.

## Sensor fusion with `RadarScan` in detection module

In addition to Camera-LiDAR sensor fusion in detection layer is proposed in [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3), I suggest Camera-LiDAR-Radar sensor fusion in detection layer.
You can select 3d detection as below.

- Camera-LiDAR-Radar 3d detection using multiple modules
- Camera-LiDAR 3d detection
- LiDAR 3d detection
- Camera 3d detection

### Camera-LiDAR-Radar 3d detection using multiple modules

Camera-LiDAR-Radar 3d detection consists of two sensor fusion modules.

- Camera-LiDAR fusion

First, detected 2d objects, detected 3d objects, and segmented LiDAR pointcloud are fused in camera-LiDAR Fusion like [roi_cluster_fusion](https://github.com/autowarefoundation/autoware.universe/tree/main/perception/roi_cluster_fusion) and [image_projection_based_fusion in the future](https://github.com/autowarefoundation/autoware.universe/pull/548).

- Radar fusion

After shape fitting, detected 3d objects from camera-LiDAR fusion and  `RadarScan` are fused in radar fusion modules.
By defining `DetectedObjects` as the input interface for camera and LiDAR data, it can improve usability for sensor fusion architecture; For example, radar fusion module can be applied to camera 3d detection.
