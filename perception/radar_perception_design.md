## Pipeline

According to the output types, I suggest two sensor fusion types as radar output.

- The radar fusion in object layer in tracking module
- The radar fusion in `RadarScan` layer in detection module

![The pipeline figure for design document](https://raw.githubusercontent.com/scepter914/autoware-radar-architecture-proposal/11501dc06651ac0acec5b4bce781fda4a2918a2f/perception/figure/perception_pipeline.drawio.svg)

This figure put on <https://github.com/scepter914/autoware-radar-architecture-proposal/blob/master/perception/figure/perception_pipeline.drawio.svg>

### The radar fusion in object layer in tracking module

In fusion tracking, radar tracked objects are fused for other tracked objects in tracking module.
Fusion tracking module handle as below objects

- LiDAR tracked 3d objects
- Camera tracked 3d objects
- Radar tracked 3d objects
- 3d objects from Camera-LiDAR-Radar fusion

Radar tracked objects are generated by

- Radar driver (Tracked object Calculated inside the radar devices)
- Radar tracking module (Calculate tracked objects from `RadarScan`)

### The radar fusion in `RadarScan` layer in detection module

In addition to Camera-LiDAR sensor fusion in detection layer is proposed in [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3), I suggest Camera-LiDAR-Radar sensor fusion in detection layer.
You can select as 3d detection

- Camera-LiDAR-Radar 3d detection using multiple modules
- Camera-LiDAR 3d detection
- LiDAR 3d detection
- Camera 3d detection

#### Camera-LiDAR-Radar 3d detection using multiple modules

 like deep learning module from images, LiDAR pointclouds, and radar scan.
