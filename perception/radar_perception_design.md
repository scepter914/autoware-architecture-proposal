## Whole Pipeline

This proposal pipeline follows [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3).
Based on [the original perception architecture figure](https://github.com/scepter914/autoware-radar-architecture-proposal/blob/main/perception/figure/perception_pipeline_original.drawio.svg) drawn using draw.io, I have expanded the sensor fusion architecture to include radar.

![The pipeline figure for design document](https://raw.githubusercontent.com/scepter914/autoware-radar-architecture-proposal/main/perception/figure/perception_pipeline.drawio.svg)

This figure can be found at <https://github.com/scepter914/autoware-radar-architecture-proposal/blob/main/perception/figure/perception_pipeline.drawio.svg>

Considering typical radar output types, I suggest two sensor fusion types in the perception module:

- Radar fusion in the object layer in the tracking module
- Radar fusion in the `RadarScan` layer in the detection module

## Radar fusion in object layer in tracking module

### Fusion tracking

In fusion tracking, radar tracked objects are fused with other tracked objects in the tracking module.
As discussed in [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3), the fusion tracking module handles tracked objects as follows:

- Detected 3D objects from Camera-LiDAR-Radar fusion
- LiDAR tracked 3D objects
- Camera tracked 3D objects
- Radar tracked 3D objects

### Radar detection and tracking

The tracked radar objects have the following features and can use for fusion tracking:

- Far object recognition
- More precise twist information estimated by doppler velocity
- Low accuracy of position and size information

Radar tracked objects are generated by two types.

![The pipeline figure for design document](https://raw.githubusercontent.com/scepter914/autoware-radar-architecture-proposal/main/perception/figure/radar_detection_tracking.drawio.svg)

- Radar driver

If you have a radar driver which can publish `ros-perception/radar_msgs/msg/RadarTrack.msg` tracked objects calculated internally by the radar devices, then you can input the radar outputs to fusion tracking.
Using the converter from from `ros-perception/radar_msgs/msg/RadarTrack.msg` to `autoware_auto_perception_msgs/msg/TrackedObject`, you can connect to fusion tracking.
Radars which can output tracked objects offer the potential to make the sensor fusion pipeline simpler with few parameters.

- Radar 3D clustering and tracking

If you have radar driver which publishes `ros-perception/radar_msgs/msg/RadarScan`, then you can use the radar 3D clustering and tracking module.
The module clusters dynamic `RadarScan` to recognize dynamic objects and tracks using doppler velocity.

## Sensor fusion with `RadarScan` in detection module

In addition to Camera-LiDAR sensor fusion in the detection layer proposed in [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3), I suggest including Camera-LiDAR-Radar sensor fusion in the detection layer.

### Camera-LiDAR-Radar 3d detection using multiple modules

Camera-LiDAR-Radar 3D detection consists of two sensor fusion stages.

- Camera-LiDAR fusion

The first stage is Camera-LiDAR fusion.
Camera-LiDAR fusion packages aim to improve class label accuracy.
Detected 2D objects and detected 3D objects are fused in Camera-LiDAR Fusion methods like [roi_cluster_fusion](https://github.com/autowarefoundation/autoware.universe/tree/main/perception/roi_cluster_fusion) and [image_projection_based_fusion in the future](https://github.com/autowarefoundation/autoware.universe/pull/548).

- Radar fusion

The second stage is radar fusion.
After Camera-LiDAR fusion, detected 3D objects from Camera-LiDAR fusion and  `RadarScan` are fused in radar fusion modules.

Radar fusion packages aim to improve detection performance using detected 3D objects with low confidence and `RadarScan`.
Radar fusion packages attach doppler velocity from `RadarScan` for detected 3D objects to improve velocity estimation in the tracking module.

By defining `DetectedObjects` as the input interface for camera and LiDAR data, it can improve usability for sensor fusion architecture; for example, the radar fusion module can be applied to the camera 3D detection.
