## Whole Pipeline

This proposal pipeline follows [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3).
Based on [the original perception architecture figure](https://github.com/scepter914/autoware-radar-architecture-proposal/blob/main/perception/figure/perception_pipeline_original.drawio.svg) drawn using draw.io, we have expanded the sensor fusion architecture to include radar.

![The pipeline figure for design document](https://raw.githubusercontent.com/scepter914/autoware-radar-architecture-proposal/main/perception/figure/perception_pipeline.drawio.svg)

This figure can be found at <https://github.com/scepter914/autoware-radar-architecture-proposal/blob/main/perception/figure/perception_pipeline.drawio.svg>

Considering typical radar output types, we suggest two sensor fusion types in the perception module:

- Radar fusion in the object layer in the tracking module
- Radar fusion in the `RadarScan` layer in the detection module

## Radar fusion in object layer in tracking module

### Fusion tracking

In fusion tracking, radar tracked objects are fused with other tracked objects in the tracking module.
As discussed in [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3), the fusion tracking module handles tracked objects as follows:

- Detected 3D objects from Camera-LiDAR-Radar fusion
- LiDAR tracked 3D objects
- Camera tracked 3D objects
- Radar tracked 3D objects

### Radar detection and tracking

The tracked radar objects have the following features and can use for fusion tracking:

- Far object recognition
- More precise twist information estimated by doppler velocity
- Low accuracy of position and size information

Radar tracked objects are generated by two types.

![The pipeline figure for design document](https://raw.githubusercontent.com/scepter914/autoware-radar-architecture-proposal/main/perception/figure/radar_detection_tracking.drawio.svg)

- Radar driver

Radar sensors capable of internally tracking and detecting objects can have their drivers modified to publish the message type `ros-perception/radar_msgs/msg/RadarTracks.msg`.
Using `radar_track_to_tracked_objects_converter` package, which converts from `ros-perception/radar_msgs/msg/RadarTracks.msg` to `autoware_auto_perception_msgs/msg/TrackedObjects`, you can connect to fusion tracking.
This output can be directly used as input to fusion tracking modules, as explained in "Fusion tracking" section.
Radars which can output tracked objects offer the potential to make the sensor fusion pipeline simpler with fewer parameters than ones which outputs only `ros-perception/radar_msgs/msg/RadarScan.msg`.

- Radar 3D clustering and tracking

Radars sensors that output raw input can adapt their drivers to publish `ros-perception/radar_msgs/msg/RadarScan`.
In this way, the clustering and tracking modules can be used to integrate these types of sensors in the fusion tracking module.

## Sensor fusion with `RadarScan` in detection module

In addition to Camera-LiDAR sensor fusion in the detection layer proposed in [Perception architecture discussion](https://github.com/autowarefoundation/autoware/discussions/3), this document proposes including radar in the detection layer.

### Camera-LiDAR-Radar 3d detection using multiple modules

The proposed Camera-LiDAR-Radar 3D detection pipeline consist of two sensor fusion stages:
Camera-LiDAR-Radar 3D detection consists of two sensor fusion stages.

- Camera-LiDAR fusion

The first stage is Camera-LiDAR fusion.
Camera-LiDAR fusion packages aim to improve class label accuracy.
Detected 2D and 3D objects are fused in the Camera-LiDAR Fusion module using methods such as [image_projection_based_fusion](https://github.com/autowarefoundation/autoware.universe/tree/main/perception/image_projection_based_fusion), which overwrites a classification label of clusters or detected objects by  that of ROIs from a 2D object detector.

- Radar fusion

The second stage is radar fusion.
After Camera-LiDAR fusion, detected 3D objects from Camera-LiDAR fusion and  `RadarScan` are fused in radar fusion modules.

Radar fusion packages aim to improve detection performance using detected 3D objects with low confidence and `RadarScan`.
Radar fusion packages attach doppler velocity from `RadarScan` for detected 3D objects to improve velocity estimation in the tracking module.

By defining `DetectedObjects` as the input interface for camera and LiDAR data, it can improve usability for sensor fusion architecture; for example, the radar fusion module can be applied to the camera 3D detection.
